'''

Wk 1 Intro 
Wk 2 Instal Anaconda, learn Python & NumPy 
Wk 3 Learn Pandas 
Wk 4 Learn Matplotlib 
Wk 5 - 6 Learn AI algo (choose 1 e.g K-NN) 
Wk 7 - 8 Data preparation 
Wk 9 - 10 Modelling 
Wk 11 Report writing 
Wk 12 Product testing 
Wk 13 Performance analysis 
Wk 14 Seminar


'''


###################################################################
# Report Writing
'''
Report cover
Abstract
Table of contents
List of figures
List of tables
List of abbreviations
1.0 Introduction
2.0 Problem statement
3.0 Project objective
4.0 Project scope
5.0 Significant of project
6.0 Data preparation (data selection, data cleansing, data transformation)
7.0 Classification model (algo/computational (how does it works), architecture, features/parameters)
8.0 Result and discussion (testing result and performance analysis)
9.0 Conclusion and recommendation
10.0 Rubric (Provided by lecturer)
References
Appendices
CD (Data, survey form if any, report, codes)
'''

###################################################################
# Python library

import matplotlib.pyplot as plt
import numpy as np
import pandas as pd
from sklearn import datasets
from sklearn.neighbors import KNeighborsClassifier
from sklearn.preprocessing import LabelEncoder
import sklearn.metrics as sm
from sklearn import metrics
from collections import Counter
%matplotlib inline


###################################################################
# Data preparation & analysis

# Load dataset
df = pd.read_csv("training.csv") #Reading the dataset in a dataframe using Pandas
df.head()

# Data analysis
Counter(df['Loan_Status'])
df.shape
df.columns
df.info()
df['ApplicantIncome'].hist(bins=50)
df.boxplot(column='ApplicantIncome')
df.boxplot(column='ApplicantIncome', by = 'Education')

plt.figure(figsize = (16,16))
green_diamond = dict(markerfacecolor='g', marker='D')
df.boxplot(column='ApplicantIncome', notch=True, flierprops=green_diamond)


temp1 = df['Credit_History'].value_counts(ascending=True)
temp2 = df.pivot_table(values='Loan_Status',index=['Credit_History'],aggfunc=lambda x: x.map({'Y':1,'N':0}).mean())
print ('Frequency Table for Credit History:') 
print (temp1)
print ('\nProbility of getting loan for each Credit History class:')
print (temp2)

plt.figure(figsize=(8,4))
plt.subplot(1,2,1)
plt.xlabel('Credit_History')
plt.ylabel('Count of Applicants')
plt.title("Applicants by Credit_History")
temp1.plot(kind='bar')

x = pd.Series(temp2['Loan_Status'])
plt.subplot(1,2,2)
plt.xlabel('Credit_History')
plt.ylabel('Probability of getting loan')
plt.title("Probability of getting loan by credit history")
x.plot(kind = 'bar')

temp3 = pd.crosstab(df['Credit_History'], df['Loan_Status'])
temp3.plot(kind='bar', stacked=True, color=['red','blue'], grid=False)

# To report missing data
df.apply(lambda x: sum(x.isnull()),axis=0) 

df['Gender'].fillna(df['Gender'].mode()[0], inplace=True)
df['Married'].fillna(df['Married'].mode()[0], inplace=True)
df['Dependents'].fillna(df['Dependents'].mode()[0], inplace=True)
df['Self_Employed'].fillna('No',inplace=True)
df['LoanAmount'].fillna(df['LoanAmount'].mode()[0], inplace=True)
df['Loan_Amount_Term'].fillna(df['Loan_Amount_Term'].mode()[0], inplace=True)
df['Credit_History'].fillna(df['Credit_History'].mode()[0], inplace=True)

# Recheck missing data
df.apply(lambda x: sum(x.isnull()),axis=0) 

# Convert to numerical
var_mod = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']
le = LabelEncoder()
for i in var_mod:
    df[i] = le.fit_transform(df[i])
df.dtypes
df.head()

df.iloc[:, 1:12]
df.loc[:, ['Loan_Status']]

x = df.iloc[:, 1:12]
y = df.loc[:, ['Loan_Status']]


# Set the size of the plot
plt.figure(figsize=(14,7))
 
# Create a colormap
colormap = np.array(['red', 'lime'])
 
# Plot Income
plt.subplot(1, 2, 1)
plt.scatter(x.ApplicantIncome, x.CoapplicantIncome, c=colormap[y.Loan_Status], s=40)
plt.title('Income')

# Plot Loan
plt.subplot(1, 2, 2)
plt.scatter(x.ApplicantIncome, x.LoanAmount, c=colormap[y.Loan_Status], s=40)
plt.title('Loan')
 

###################################################################
# Modelling

# K-Nearest Neighbor
model = KNeighborsClassifier(algorithm='auto', leaf_size=30, metric='minkowski', metric_params=None, n_jobs=None, n_neighbors=2, p=2, weights='uniform')
model.fit(x, y)
yPred = model.predict(x)
# small k lead to more complex model and OF

# View the results
# Set the size of the plot
plt.figure(figsize=(14,7))
 
# Create a colormap
colormap = np.array(['red', 'lime'])
 
# Plot the Original Classifications
plt.subplot(1, 4, 1)
plt.scatter(x.ApplicantIncome, x.CoapplicantIncome, c=colormap[y.Loan_Status], s=40)
plt.title('Income: Real Classification')
 
# Plot the Models Classifications
plt.subplot(1, 4, 2)
plt.scatter(x.ApplicantIncome, x.CoapplicantIncome, c=colormap[yPred], s=40)
plt.title('Income: K-NN Classification')

# Plot the Original Classifications
plt.subplot(1, 4, 3)
plt.scatter(x.ApplicantIncome, x.LoanAmount, c=colormap[y.Loan_Status], s=40)
plt.title('Loan: Real Classification')
 
# Plot the Models Classifications
plt.subplot(1, 4, 4)
plt.scatter(x.ApplicantIncome, x.LoanAmount, c=colormap[yPred], s=40)
plt.title('Loan: K-NN Classification')


###################################################################
# Testing & performance analysis -Trainning data

# Seaborn #Run at new python 3
import seaborn as sns
plt.figure(figsize=(9,9))
sns.heatmap(sm.confusion_matrix(y, yPred), annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(sm.accuracy_score(y, yPred))
plt.title(all_sample_title, size = 15);


###################################################################
# Testing & performance analysis -Testing data

df2 = pd.read_csv("/Users/khairul/Desktop/kYan Syamsul Nurul CSC649 shared/test.csv") 

df2['Gender'].fillna(df2['Gender'].mode()[0], inplace=True)
df2['Married'].fillna(df2['Married'].mode()[0], inplace=True)
df2['Dependents'].fillna(df2['Dependents'].mode()[0], inplace=True)
df2['Self_Employed'].fillna('No',inplace=True)
df2['LoanAmount'].fillna(df2['LoanAmount'].mode()[0], inplace=True)
df2['Loan_Amount_Term'].fillna(df2['Loan_Amount_Term'].mode()[0], inplace=True)
df2['Credit_History'].fillna(df2['Credit_History'].mode()[0], inplace=True)

var_mod2 = ['Gender','Married','Dependents','Education','Self_Employed','Property_Area','Loan_Status']
le2 = LabelEncoder()
for i in var_mod2:
    df2[i] = le2.fit_transform(df2[i])

x2 = df2.iloc[:, 1:12]
y2 = df2.loc[:, ['Loan_Status']]
yPred2 = model.predict(x2)

# Seaborn #Run at new python 3
import seaborn as sns
plt.figure(figsize=(9,9))
sns.heatmap(sm.confusion_matrix(y2, yPred2), annot=True, fmt=".3f", linewidths=.5, square = True, cmap = 'Blues_r');
plt.ylabel('Actual label');
plt.xlabel('Predicted label');
all_sample_title = 'Accuracy Score: {0}'.format(sm.accuracy_score(y2, yPred2))
plt.title(all_sample_title, size = 15);


###################################################################
#K-NN parameters description
'''
n_neighbors : int, optional (default = 5)
Number of neighbors to use by default for kneighbors queries.

weights : str or callable, optional (default = ‘uniform’)
weight function used in prediction. Possible values:
‘uniform’ : uniform weights. All points in each neighborhood are weighted equally.
‘distance’ : weight points by the inverse of their distance. in this case, closer neighbors of a query point will have a greater influence than neighbors which are further away.
[callable] : a user-defined function which accepts an array of distances, and returns an array of the same shape containing the weights.

algorithm : {‘auto’, ‘ball_tree’, ‘kd_tree’, ‘brute’}, optional
Algorithm used to compute the nearest neighbors:
‘ball_tree’ will use BallTree
‘kd_tree’ will use KDTree
‘brute’ will use a brute-force search.
‘auto’ will attempt to decide the most appropriate algorithm based on the values passed to fit method.
Note: fitting on sparse input will override the setting of this parameter, using brute force.

leaf_size : int, optional (default = 30)
Leaf size passed to BallTree or KDTree. This can affect the speed of the construction and query, as well as the memory required to store the tree. The optimal value depends on the nature of the problem.

p : integer, optional (default = 2)
Power parameter for the Minkowski metric. When p = 1, this is equivalent to using manhattan_distance (l1), and euclidean_distance (l2) for p = 2. For arbitrary p, minkowski_distance (l_p) is used.

metric : string or callable, default ‘minkowski’
the distance metric to use for the tree. The default metric is minkowski, and with p=2 is equivalent to the standard Euclidean metric. See the documentation of the DistanceMetric class for a list of available metrics.

metric_params : dict, optional (default = None)
Additional keyword arguments for the metric function.

n_jobs : int or None, optional (default=None)
The number of parallel jobs to run for neighbors search. None means 1 unless in a joblib.parallel_backend context. -1 means using all processors. See Glossary for more details. Doesn’t affect fit method.

'''
